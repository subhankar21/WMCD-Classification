{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 398 entries, 350 to 337\n",
      "Data columns (total 30 columns):\n",
      "radius_mean                398 non-null float64\n",
      "texture_mean               398 non-null float64\n",
      "perimeter_mean             398 non-null float64\n",
      "area_mean                  398 non-null float64\n",
      "smoothness_mean            398 non-null float64\n",
      "compactness_mean           398 non-null float64\n",
      "concavity_mean             398 non-null float64\n",
      "concave points_mean        398 non-null float64\n",
      "symmetry_mean              398 non-null float64\n",
      "fractal_dimension_mean     398 non-null float64\n",
      "radius_se                  398 non-null float64\n",
      "texture_se                 398 non-null float64\n",
      "perimeter_se               398 non-null float64\n",
      "area_se                    398 non-null float64\n",
      "smoothness_se              398 non-null float64\n",
      "compactness_se             398 non-null float64\n",
      "concavity_se               398 non-null float64\n",
      "concave points_se          398 non-null float64\n",
      "symmetry_se                398 non-null float64\n",
      "fractal_dimension_se       398 non-null float64\n",
      "radius_worst               398 non-null float64\n",
      "texture_worst              398 non-null float64\n",
      "perimeter_worst            398 non-null float64\n",
      "area_worst                 398 non-null float64\n",
      "smoothness_worst           398 non-null float64\n",
      "compactness_worst          398 non-null float64\n",
      "concavity_worst            398 non-null float64\n",
      "concave points_worst       398 non-null float64\n",
      "symmetry_worst             398 non-null float64\n",
      "fractal_dimension_worst    398 non-null float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 96.4 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "(398, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy svm: 60.819%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Subhankar Chattoraj\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data = pd.read_csv(\"../autoencoder_Python/data.csv\",header=0)\n",
    "data.drop(\"Unnamed: 32\",axis=1,inplace=True)\n",
    "data.drop(\"id\",axis=1,inplace=True)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "train, test = train_test_split(data, test_size = 0.3)\n",
    "\n",
    "prediction_var = list(data.columns[1:31])\n",
    "outcome_var = \"diagnosis\"\n",
    "\n",
    "train_X = train[prediction_var]# taking the training data input\n",
    "train_y = train.diagnosis# This is output of our training data\n",
    "# same we have to do for test\n",
    "test_X = test[prediction_var] # taking test data inputs\n",
    "test_y = test.diagnosis   #output value of test dat\n",
    "\n",
    "train_X.info()\n",
    "import matplotlib.pyplot as plt\n",
    "n_train = np.array(train_X)\n",
    "n_test = np.array(test_X)\n",
    "\n",
    "plt.plot(n_train)\n",
    "plt.show()\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "input_dim = n_train.shape[1]\n",
    "feature_dim = [25, 20, 15, 10]\n",
    "print(input_dim)\n",
    "inputs = Input(shape=(input_dim,))\n",
    "encoded = inputs\n",
    "encoded = Dense(feature_dim[0], kernel_initializer=\"uniform\")(encoded)\n",
    "encoded = Dense(feature_dim[1], kernel_initializer=\"uniform\")(encoded)\n",
    "encoded = Dense(feature_dim[2], kernel_initializer=\"uniform\")(encoded)\n",
    "encoded = Dense(feature_dim[3], kernel_initializer=\"uniform\")(encoded)\n",
    "\n",
    "decoded = encoded\n",
    "decoded = Dense(feature_dim[2], kernel_initializer=\"uniform\")(decoded)\n",
    "decoded = Dense(feature_dim[1], kernel_initializer=\"uniform\")(decoded)\n",
    "decoded = Dense(feature_dim[0], kernel_initializer=\"uniform\")(decoded)\n",
    "decoded = Dense(input_dim, kernel_initializer=\"uniform\")(decoded)\n",
    "\n",
    "\n",
    "autoencoder = Model(inputs, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='mse')\n",
    "\n",
    "autoencoder.fit(n_train, n_train,\n",
    "                verbose=0,\n",
    "                epochs=150,\n",
    "                batch_size=100,\n",
    "                shuffle=True,\n",
    "                validation_data=(n_test, n_test))\n",
    "\n",
    "predict_vals = autoencoder.predict(n_train)\n",
    "print(predict_vals.shape)\n",
    "plt.plot(predict_vals)\n",
    "plt.show()\n",
    "from keras.models import Sequential\n",
    "\n",
    "featuremodel = Sequential()\n",
    "featuremodel.add(Dense(feature_dim[0], input_shape=(input_dim,), weights=autoencoder.layers[1].get_weights()))\n",
    "featuremodel.add(Dense(feature_dim[1], weights=autoencoder.layers[2].get_weights()))\n",
    "featuremodel.add(Dense(feature_dim[2], weights=autoencoder.layers[3].get_weights()))\n",
    "featuremodel.add(Dense(feature_dim[3], weights=autoencoder.layers[4].get_weights()))\n",
    "\n",
    "featuremodel.compile(optimizer='adadelta', loss='mse')\n",
    "\n",
    "from sklearn import svm # for Support Vector Machine\n",
    "from sklearn import metrics # for the check the error and accuracy of the model\n",
    "\n",
    "# classic svm\n",
    "model = svm.SVC()\n",
    "model.fit(train_X,train_y)\n",
    "prediction=model.predict(test_X)\n",
    "print(\"Accuracy svm: %s\" % \"{0:.3%}\".format(metrics.accuracy_score(prediction, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
